{
  "Emperor angelfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Avery Fisher Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Set: Burning of the Euromaidan headquarters": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Cardinal Niccolò Albergati": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Kapellbrücke": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Maria Isabel of Portugal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Chocolate Brownie": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "John Constable - Wivenhoe Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ellen Axson Wilson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Oblique shock wave": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Aiding a Comrade": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Juniperus brevifolia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Fallingwater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Frankfurt (Main) Hauptbahnhof": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Daedongyeojido": {
    "yes": [
      "gpt-4o-2024-05-13",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The Sisters (Eleanor and Rosalba Peale)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Oscar Niemeyer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Coat of arms of Mexico": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Piring Dance Attraction": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Green iguana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "1956 Texas Official Travel Map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Anthracoceros coronatus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Egg fruit cross section DS.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gurabija (гурабија, Qurabiya) part of Balkan, Arab, Ottoman cuisine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dunnock": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Common moorhen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hudsonian Godwit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "I-35W Mississippi River bridge collapse": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Ho Chi Minh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Somapura Mahavihara": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Equestrian Portrait of Charles V": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Marcos visit Johnson 1966": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dorf in den Berner Alpen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Red-tailed laughingthrush": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chalcolestes viridis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vincent van Gogh (Russell painting)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Strep throat": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "Osama Bin Laden's Safehouse": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Syed Kirmani": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Bust of Redmond Barry": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Rocket": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Taj Mahal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Blue-tailed damselfies mating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Oktoberfest": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Glardon Vallorbe needle file set": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Battle of Wimpfen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Wild strawberry": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "the Māori Battalion doing a haka": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Point Montara Light": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cephalanthus occidentalis occidentalis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Church of the Redeemer (Toronto)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Victoria (Later Queen Victoria) with her spaniel Dash, 1833. Painting by George Hayter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Javan slow loris": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Battle of Franklin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Edward Mitchell Bannister": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "George Washington Carver": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Danafungia scruposa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Small red damselfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Costumes for William Tell": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Rooftop farm at the Essex": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gorce Mountains": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sunset on Pluto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Neural map of a giant scallop": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Olive ridley sea turtle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Nadar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Portuguese real – 2400 réis (1798–99)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Roger Chaffee in Mission Control": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Alhambra from Mirador San Nicolás": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Imam Reza shrine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "A word from the Rear Admiral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Basilica of Saint Peter in Rome": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Sound Suppression System for Space Shuttle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Gismonda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Thelenota ananas": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Niels Bohr": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Roses of Heliogabalus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Spotted Wood Owl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Golden-headed Cisticola": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Albert Reiss": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Company store in Lynch, Kentucky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Laura Dekker": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Ussher's palla": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "African blue flycatcher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Salacca wallichiana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Wildebeest on migration in East Africa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Leheriya Gate, City Palace, Jaipur": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Stained glass": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "The Plum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Trollius europaeus seedhead": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bowman Creek": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dutch - Flower Still Life": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Langtang National Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Bill Hosokawa's home at the Heart Mountain Relocation Center ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Pope Pius VII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Russian Invasion of Ukraine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Man versus fish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Arena of Nîmes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "K. Babu (Minister)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Impossible color fatigue template": {
    "yes": [
      "gpt-4o-2024-05-13",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Garden locust": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Stained-glass windows, church of San Jeronimo Real": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Map of the night march from Malvern Hill to Harrison's Landing - Battle of Malvern Hill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Globular cluster Messier 69": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Parboiled rice with chicken": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sphinx moth, (Adhemarius donysa)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cinnamon redux": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "File:Salisbury Cathedral Lady Chapel 2, Wiltshire, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sherman-Grant note": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "National Archaeological Museum, Athens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Yacare caiman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Assut de l'Or Bridge and L'Àgora,": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Palácio Quitandinha": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Berca Mud Volcanoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Copenhagen Opera House": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Leptura quadrifasciata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Three Brothers jewel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "A van Gogh selfie": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Curiosity rover": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Broderie Room at the Phipps Conservatory": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Landgericht Berlin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hall of Names": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Battle of Borodino": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Ovenbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ortolan bunting": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Amphipoea oculea": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pauline Adams redux": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Proboscis monkey jumping (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Pool of Bethesda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Impact sprinkler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Roman Forum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Barack Obama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Exchequer note, 5 pounds, Kingdom of England (1697)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Netherlands American Cemetery (memorial tower)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eurasian eagle-owl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Carolina wren": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Narmer Palette": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Abbey of Saint-Michel-de-Cuxa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Effects of cache partitioning": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Clitocybe nebularis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "San Pedro and San Pablo volcanoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "SAI KZ IV": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Baptismal site of Jesus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Alva Belmont": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Saung-Gauk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hummingbird hawk moth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "George Michael": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Cirsium palustre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The International": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Man": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Parisian Life": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Challenger 2 Tank Firing": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The great comet of 1881": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Original France XV": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Stargazer and Pegasus F43 in flight over the Atlantic": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Saltwater crocodile": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Financial District, Lower Manhattan: New York.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "File:San Lorenzo (Turin) - Dome interior": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Trimeresurus popeiorum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Stitching the Standard": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Thermoclines and jellyfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pyotr Bagration": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cardiss Collins": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Juma Mosque, Shamakhi (2021)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Washington State Capitol": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Bidwell Mansion State Historic Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Venus Consoling Love, François Boucher, 1751": {
    "yes": [
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Theodore von Kármán at Arnold Air Force Base": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Chapel Royal, Dublin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pope Pius XII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Jollof Rice with Stew": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Aida: Act IV, Scene 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bird-cherry ermine moth nest of caterpillars": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The perfect galactic wirlpool": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "James A. Garfield": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Event Horizon Telescope and Apollo 16": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Sherlock Holmes (play)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Portrait of Muhammad Dervish Khan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Variegated grasshopper": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Plexippus paykulli": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Paris Street; Rainy Day": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Assumption of Mary": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pink-eared Duck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Norrie State Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Avian flu vaccine development by reverse genetics": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Delftsevaart in Rotterdam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Swiss Federal Council": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Design for The Cenotaph": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Rubens Peale with a Geranium by Rembrandt Peale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "South Georgia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Stephanie Wilson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Laocoön (El Greco)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Elizabeth Smith-Stanley, Countess of Derby": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Perseus and Andromeda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Trisopterus luscus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Interior of the Palm House on the Pfaueninsel Near Potsdam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Copper sunbird female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pancuran Tujuh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hieronymus Bosch, The Crucifixion of St Julia triptych, c.1497-1505": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mediterranean red sea star": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Kiwifruit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pakistan Monument": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Väike-Maarja church": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Epipactis palustris": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Alpine ibex": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Iris Pruysen long jump": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Caltha palustris flower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gliophorus chromolimoneus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Galah": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Julia Margaret Cameron": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Jupiter's southern storms": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Talismanic shirt bearing the 99 names of God": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lucy Arbell as Queen Amahelli in Massenet's Bacchus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Fall of the Titans": {
    "yes": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "MS Marina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vienna State Opera": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Le Moulin de la Galette": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Rainbow White House": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Agbogbloshie": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "European mantis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pabda macher jhal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mourning of Muharram in Iran": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Juvenile hawfinch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "\"Māda\" (Median) herald leading a delegation on the famous tribute bearers bas-relief decorating the southern panel of the eastern stairway of the Apadana, Darius the Great’s audience hall at Persepolisriteria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Monticello": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "SMS Gazelle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Kármán vortex street": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Chex Mix": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Estadio Azul": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "File:Magdalene College Dining Hall, Cambridge, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "C-class Melbourne tram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Lōdal Evo T-28 waste collection truck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Yokugo no onna by Goyō Hashiguchi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Schloss Münster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Siege of Tsingtao, 10 Japanese sen (1914)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Wreck of Cabo de Santa Maria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "John Ford": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Tito Karnavian": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Self-Portrait with Halo and Snake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Barack Obama with full cabinet 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Johann Jakob Frey: Lepsius Expedition to Egypt: Raising of the Prussian flag on the Great Pyramid of Giza": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Polarized light microscopy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Newar Woman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Autumn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mount Everest 3D map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hubertine Auclert": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Les murmures de l'Amour": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sorrow (Van Gogh)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Spanish Civil War (1936) mass grave, with 26 people.": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Slaty-crowned antpitta": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Male jaguar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Masih Alinejad": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Notre-Dame Basilica, Montreal - Exterior": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eileen Collins": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jennifer Doudna": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Christ Pantocrator, Church of the Holy Sepulchre, Jerusalem": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Cité de Carcassonne": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Chevrotain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Saturnia pavonia caterpillar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Selfoss waterfall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Temple of Caitanya Mahaprabhu in Mayapur": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Silver moony": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "July 2018 lunar eclipse": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "South Melbourne Town Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Autumn leaf butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Angklung": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Rescue swimmer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Snakelocks anemone": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Common kingfisher female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Messier 106": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Walking on a Mountain Path in Spring": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Della H. Raney": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "North American B-25 Mitchell": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Roses - P.S. Krøyer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Trundholm sun chariot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Justice Bechu Kurian Thomas": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Toompea Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Venus and Anchises": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Nithya Menen Actress": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "al-Wakwak": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Common Blue": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Joshua passing the River Jordan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sulfur extraction": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Larabanga Mosque": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Eastern Spinebill Male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Khone Phapheng Falls": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vysotsky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Snowy egret": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chesterton Windmill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Madonna of the rocks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "La Schiavona": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Germaine de Staël": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Groundscraper Thrush": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "USS Constitution fires a 17-gun salute": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gentoo penguin (Brown Bluff, Antarctica)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ethel Smyth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Carling Black Label": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "SS Carnatic": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "One Hundred Horses": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Nikola Tesla, Colorado Springs Laboratory (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Eva Nogales": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Falcon 9 Full Thrust": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Panorama of Victoria Harbour": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Comb": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Françoise-Marguerite de Sévigné": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Four Continents": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mount Merapi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Marie Curie": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Philip Wharton, 4th Baron Wharton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Woman with Mirrors": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The 1901 \"Basket of flowers\" egg by Peter Carl Fabergé": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Chickahominy - Sumner's Upper Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "The garden of Eden with the fall of man": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rosencrantz and Guildenstern": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "State Fair": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pyrite (2021)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sagunto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Blind-Man’s Buff by Jean-Honoré Fragonard (1750 - 1752)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ayasofya Mosque (1852)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Lake Balaton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Evening Air": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Peep show in Golestan Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "File:Fountains Abbey, Yorkshire, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pepper No. 30": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Einstein 1921 by F Schmutzer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Ma, Ma, where's my Pa?": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Shangrila Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Rosa Parks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Samuel Beckett by Edmund S. Valtman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Jessica Meir": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Teatro Grand a baroque Theater in Brescia Italy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Point Arena Light": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "George I Rákóczi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Sari temple": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Engaged": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Les Invalides": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Faroese sheep": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chestnut-naped antpitta (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Moss campion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Viperine water snake (Natrix maura)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Vexilla vexillum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Shōrin-zu byōbu (Pine Trees screen) (two panels)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Adélie penguin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Men's sabre semi-finals of the 2013 World Fencing Championships": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Eastern quoll (Dasyurus viverrinus) fawn morph Esk Valley.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Green monkey (Chlorocebus sabaeus) juvenile head.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Portrait of an Iguana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cerastes cerastes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Oriental hornet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Louis Meijer - Selfportrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Charon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Scotland Forever!": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "A Composite Imaginary View of Japan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Spotted trunkfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Vreten metro station, Stockholm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "File:Balliol College Dining Hall, Oxford - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Set design for Act V, Scene 2 of Fromental Halévy's La reine de Chypre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dennis Schröder": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Key Monastery, Spiti Valley, India": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Rathgall Hillfort": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Forum Romanum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Bactrian princess": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Rape of the Daughters of Leucippus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cepelinai": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Horse Fair": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Phidippus otiosus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pair of Mandarin ducks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mercury City Tower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Stephen Hawking in zero gravity": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Luidia senegalensis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The stolen kiss by Jean-Honoré Fragonard (late 1780s)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Floating market": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Owl moth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hurricane Ivan from the ISS": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Stonewall Inn \"raided premises\" sign": {
    "yes": [
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Independence or Death": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Phaistos Disc": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Galena": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Uncle Sam's new class in the art of self-government": {
    "yes": [
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Temple of Garni": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Southern pale chanting goshawk juvenile": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Still Life with Flowers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pycnocline": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Palm Beach County Park Lantana Airport ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Portrait of a Young Woman (Vermeer)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Thomas Müller": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Moros, Zaragoza": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Nils Olav": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Kill Count": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Qurabiya": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Kingdom of Norway, 5 Rigsdaler Courant (1807)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Nintendo Switch Image": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Francine Jordi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mezquita Shah, Isfahán, Irán": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Madame de Pompadour": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "National Museum of Natural History (France)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Château de Luynes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Common brimstone butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Alaskan parchment scrip, 1 ruble (c. 1852)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Long-billed curlew": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Angela Davis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Study for Cardinal Niccolò Albergati": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Milkmaid": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Asia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Male black darter dragonfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Umm al-Fahm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "George Grossmith as Bunthorne in Patience": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Fannie Lou Hamer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Change in average temperature over the last 50 years": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Recreation Yard (Alcatraz)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Wright brothers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Lilly Walleni": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Space Selfie": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ice bucket challenge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mwambutsa IV Bangiricenge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Nautilus anatomy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Holothuria arguinensis (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Paris pissoir in 1865": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Coldstream Guards in the Crimean War": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Germany–Poland border": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "John Henry Turpin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Echo and Narcissus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Northern Gannet with nest material": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Jog Falls, India": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Smoke grenade": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "HR 8799": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Helen Keller circa 1920": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Aircraft Rescue Firefighting training": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jade vine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "White-cheeked starling": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Signing of Paris Peace Accords": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Wedding Supper in the Hofburg Palace, in the Redoute Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Progressive Views Of Comet Kohoutek": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Camas pocket gopher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Australian Raven": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Emmy Noether": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Coenonympha glycerion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ntozake Shange": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Marsh wren": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Danish West Indian 10 daler gold coin (1904)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Magdalen College Oxford Old Grammar Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Henrietta Rodman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Theodore Roosevelt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mahuri": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Wat Arun at night": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Crystal Eastman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Little corella": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Belgium, 40 Francs (1835)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Botticelli's Mystical Nativity": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Greenfield tornado": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Red Hawk cheese": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Le mage": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Ratification of the Treaty of Münster, 15 May 1648.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Kaleva Church": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Crested myna": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "All Saints, Margaret Street London Interior": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "File:Barsana Holi Festival.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "NGC 2818": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mount Ararat Panorama (2nd attempt)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "United States gold coins (III) – Three-dollar piece (1854–89)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "American troops of the 28th Infantry Division march down the Avenue des Champs-Élysées, Paris, in the `Victory' Parade.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mars rovers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Fred M. Vinson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Lamborghini Aventador": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Guilty Mother": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Tooth and Tail": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "\"Tiny Planet\" of the Last Supper sculpture in Esino Lario at Wikimania 2016": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Hubble Ultra Deep Field 2014 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Charles Follen McKim by Frances Benjamin Johnston": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Carrie Chapman Catt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Horatius Cocles": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Oia, Greece": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sikhanyiso Dlamini": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Laura Clay": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Ceiling of the Hall of the Consistory in Palazzo Pubblico (Siena)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Chechen rebels down an Mi-8 helicopter": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "The Peasant Wedding": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Leo Tolstoy 1897": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Le roi d'Ys": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Little Owl (Athene noctua lilith) in south Hebron, (WestBank), Palestine.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vilnius Old Town life": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "File:St James's Church Interior 2, Spanish Place, London, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Emperor's speech (Pedro II of Brazil in the opening of the General Assembly)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Loch Lomond landscape": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Closeup of the center of a stone wheel - Konark Sun Temple, Orissa, India": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Carcassonne": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ulysses S. Grant": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Leo Tolstoy by Sergey Prokudin-Gorsky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Langlois Bridge at Arles": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Werther": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Celestine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Vaccinium ovalifolium": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Among the Sierra Nevada, California": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Kirinia roxelana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Nanhaipotamon hongkongense": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Johann Rupert": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Green shield bug": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "De Viron Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vermeer struck the right chord with the lady": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Stockholm City Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Kue gapit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Haboku sansui (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Sheldonian Theatre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Halftone diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Haile Selassie I official portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Typhoon Noru of 2017 at its peak intensity": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Trooping the Colour": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chicken egg diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Pancreatic cancer metastasised to the liver": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Serena Williams": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Male European stonechat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Giant kingfisher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Golden gate of Ueno Tōshō-gū": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Hamadryas baboon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sikhanyiso Dlamini and Temtsimba Dlamini": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Greater blue-eared starling": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Flower Still Life": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of John Adams by Gilbert Stuart": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Isfahan Lotfollah mosque ceiling symmetric": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Marocaster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sergeant John Geary, Thomas Onslow and Lance Corporal Patrick Carthay of the 95th (Derbyshire) Regiment of Foot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Quercus robur acorns": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Christina Nilsson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Alexander Gardner": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Portrait of Count Stanislas Potocki": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Fork-tailed flycatcher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Magna Lykseth-Skogman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Place du Capitole": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bladder wrack": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Heart Nebula": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Black-sided hawkfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Terra Ronca State Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Stade Français playing Racing Club de France": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Danaus genutia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "2019 coronavirus (SARS-CoV-2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Structures of phospholipids in aqueous solution": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Peripheral drift illusion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Chaco chachalaca": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Madama Butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Polish Hamlet - Portrait of Aleksander Wielopolski": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Because flowers and chocolates weren't grand enough": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Simon Boccanegra": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "F-15EX Eagle II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Abhayagiri Dagoba": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Jacques-Louis David Self Portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Pristimantis elegans frog": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Samuel Peploe - Still life with Apples and Jar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Eurasian bullfinch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Vischering Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Widtsoe, Utah": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Greater shearwater (Ardenna gravis) in flight Sagres.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rowland Buckstone and Cissy Grahame in the revival of F. C. Burnand's The Colonel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Dark purple Trichoglottis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Portrait of a Carthusian": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Aurora Borealis over Bear Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Black-crowned Barwing": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Katherine Wallace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Lens Olympus Zuiko OM 50 mm f/1.8": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Huacachina Dunes Oasis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gastric mucosa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Blue Marble": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eureka Inn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Basilica of St Paul, Rabat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "ΔFosB": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Golden monkey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Daenerys": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Nettle tree butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Burnside's Bridge at the Battle of Antietam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Crew of an M-24 Chaffee Tank in Korea": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Samango monkey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rockwarbler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Constanța Casino": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Grivet (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Italian States, Venice, 50 Zecchini (c. 1779–89)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "della Francesca's Baptism of Christ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Gross Clinic": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Vermeer's astronomer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "LBJ meets with Ferdinand Marcos in Manila 1966-10-23": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "File:European Parliament Strasbourg Hemicycle - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Elektrozavodskaya (Moscow Metro)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Highland cattle Bos (primigenius) taurus, bull, cow and calf on mount Secëda in Val Gardena": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Esabelle Dingizian": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Astronaut training": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Oligodon huahin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Emu": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "File:Wells Cathedral West Front Exterior, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Heungseon Daewongun": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Marine flatworm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Barge Haulers on the Volga by Ilya Repin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Laetiporus sulphureus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "A Young Woman standing at a Virginal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Eurasian nuthatch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Doris": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Nazca Lines": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Multicoloured volcanic sublimates on altered rock, Mutnovsky volcano, Kamchatka": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Temple of Isis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Granada cathedral panorama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Buff-breasted Paradise-Kingfisher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Western bowerbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Erythronium americanum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sand particles": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Julie d'Aubigny": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "A  young Lady and a Unicorn ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jasmund National Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Panther chameleon male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Comet C/2013 US10 Catalina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Brown-eared bulbul": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Katharine Hepburn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ivor Novello": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mating Skippers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hyles gallii - caterpillar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Green frog": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sonar image": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Chien-Shiung Wu": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Holmdel Horn Antenna": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Chevelle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Steve Jobs announces the iPhone 4": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mosaic of Justinianus I - Basilica San Vitale (Ravenna)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Frances Benjamin Johnston": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "St. Joan of Arc Chapel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Side view of Patenga sea beach": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "William Utermohlen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Giorgione - Young bride - Laura": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Waiting (Degas)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Nymphaea Laydekeri Purpurata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Frederick Fleet, Titanic crew and survivor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Male springbok": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "vG's Bedroom": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Leica II, front view, 1931": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Skull with cigarette by Vincent van Gogh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Australian King Parrot male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Bjørnstjerne Bjørnson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Marsh fritillaries mating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "AG Carinae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Justice Sukumaran": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Llama mother and cria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sarah Bernhardt, par Nadar, 1864": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Crab (Pachygrapsus marmoratus) on Istrian coast": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "NGC 1097 (2nd nomination)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gopuram of Tungnath Mandir": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Anna Bartels": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "A cattle farm in Borongan City, Eastern Samar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Luzon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Jupiter, Neptune and Pluto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Stargazer snake eel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Poznań Town Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "A mosque in Karachi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "John Lorimer Worden with the Tiffany & Co sword": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Angelica Kauffmann": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pula Arena inside": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Einat Kalisch-Rotem": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Algar do Carvao": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Martial eagle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Lasiocampa quercus caterpillar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Special operations parachuting": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Muhammadu Buhari": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ruth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Esperanza Base, Argentine Antarctic Research Station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Manners of articulation": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Dawn of a new age: SpaceShipOne, taxi's into the history books, the first of two flights, 16P.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "BASE jumping from tower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Large Gautama Buddha statue in Buddha Park of Ravangla, Sikkim": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dorothy Anstett": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tornado outbreak of March 31 – April 1, 2023": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Hypomeces squamosus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Persicaria maculosa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of Doge Leonardo Loredan (1501–02)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Geoffroy's tamarin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Saint Isaac's Cathedral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Franklin D. Roosevelt, by Vincenzo Laviosa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Geology of the Himalayas, seen in Mud village, Spiti": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Södermanland Runic Inscription 113": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Rough diamonds in UV and normal light": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "SMS Kaiser Wilhelm II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Herman Willem Daendels": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Tony Lovato": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Lovely abalone snail": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pope Julius II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Leland D. Melvin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Carlo Maderno Fountain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Martin Luther King": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Firewheel (Gaillardia pulchella) in Aspen, Colorado": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Allium rothii": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "King Girvan Yuddhavikram Shah": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Goalkeeper (association football)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hollow Horn Bear": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Gordon Tobing": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Red cat snake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "You forget, but she remembers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Fatima Masuma Qum Dome": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pauline Kirby": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Koch – Mayor of the City of New York": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Kheer Bhawani Temple": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Julia Shaw": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Northern Lights Cathedral, Alta": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Happy Chandler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Sunita Williams": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Saladin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Berliner Zeitung": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sri Mariamman Temple Singapore": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jade Raymond": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "RAF Cardington Sheds": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dirt jumping": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Christiansborg Slot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Some account of the bloody deeds of General Andrew Jackson, circa 1828": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Orange Oranda goldfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Margaret Harwood": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Orbicular batfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Quince Blossom": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sagittarius A*": {
    "yes": [
      "gpt-4o-2024-05-13",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The Denial of St. Peter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "2023 Fagradalsfjall eruption": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hollywood sign": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Fervaal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mir Castle Complex in Belarus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "18th Street \"L\" station, Chicago, 1904": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Pencil sharpener": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Curug Cipendok": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Westminster Bridge with the Westminster Palace in the background": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Malabar pied hornbill in flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Balzhinima Tsyrempilov": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mesa in Noctis Labyrinthus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Flatid leaf bugs and nymphs": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Kriéger Landaulette": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Convair X-6": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Las Meninas (1656)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Aldine Square, Chicago": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Toco toucan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "IceCube Neutrino Observatory": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Marion Steam Shovel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Panyembrama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Krestovsky Stadium, Saint Petersburg, Russia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Lady Cockburn and Her Three Eldest Sons": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Red-whiskered bulbul": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Song sparrow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Heic 0609": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Silver spotted skipper": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cacatua galerita at an Australian park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Boyd's forest dragon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "CG Heart": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Henry Wallis- The Death of Chatterton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Laura Bush": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Crimson pitcherplant": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hidatsa Dog Dancer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ponta Delgada City Hall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Phelsuma grandis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "ATS spotter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sun Set over Phewa Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Naqsh-e Jahan Square": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Nelly Martyl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "John Murray, 4th Earl of Dunmore": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Plexippus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Drypetes sepiaria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Maiden (Klimt)": {
    "yes": [],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 1
  },
  "THE VIEW (Virtual Reality)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Red-capped robin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Atacamite": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Valère Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pomelo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rosa Raisa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Eastern grey kangaroo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Blue-throated macaw": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Booker T. Washington": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Strip photography (Cablecar)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Costa Rica, 20 Colónes (1897)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Cymon and Iphigenia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Meiosis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Komodo dragons fighting": {
    "yes": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Cape ground squirrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Coconut": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Potsdam Day": {
    "yes": [
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Dimitri": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "William Henry Seward by Randolph Rogers, in Madison Square Park, New York City": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Lorde": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Katse Dam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Paper wasp in nest": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ruth Bader Ginsburg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Ernest Deane": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Hou Yifan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Manneken Pis 2": {
    "yes": [
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Minggu Pagi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Alice Paul": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Red-lored whistler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Passion of Joan of Arc (1928) English Poster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Kenje Ogata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "File:Petra Martic 1, Wimbledon 2013 - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Challenger's final crew": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Alessandro Martinelli": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Peter Paul Rubens - Abundance": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Julian Alps": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Black-necked stilt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Charles De Gaulle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The Jewish Cemetery": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Shaggy scalycap": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Haridwar Junction railway station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Pinatubo eruption": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Robert E. Lee": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Palace of Fine Arts": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sveti Stefan, Montenegro.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Olive-bellied sunbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Berlin Hauptbahnhof": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Clarinet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Poster for the Seventh Conference of the International Woman Suffrage Alliance": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Minnie Maddern Fiske": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Bronze-winged jacana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Basilica of SS. Ulrich and Afra, Augsburg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "STS-129 Atlantis Ready to Fly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sojourner Truth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Discobolus in National Roman Museum Palazzo Massimo alle Terme": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Skibobbing": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "P. L. Travers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Lawrence Hogan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Violet flower in Lithuania": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Water Drop": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of Mrs Richard Yates": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dynjandi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Snowy owl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Portrait of Henry VIII ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Red Cape": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Kongouro from New Holland": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Monument to Alfonso XII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Christoph Willibald Gluck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Stratosphere from aircraft": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Emmeline Pankhurst": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Cinnamon quail-thrush": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tarazona": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Finding of the Saviour in the Temple": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Vera Songwe": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Leotia lubrica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bas-relief in Persepolis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Madame de Genlis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pillar coral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "St. Mary's Syro-Malabar Cathedral Basilica, Ernakulam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "View of MBS from the gardens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Roaring Creek in wintertime": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Talitrus saltator": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "The Sound of Music": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Pena National Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sarah Bernhardt, par Nadar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Crepidotus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "John Meintz, punished during World War I": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Their New Jerusalem": {
    "yes": [
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Tsarap Valley near Phugtal Monastery": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Underwater diving": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Quarrel of Oberon and Titania": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Handmade brooms": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Tom Taylor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Citation needed": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Mary Jackson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Desert rose (crystal)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Nowa Ruda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Anny Duperey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Nina Sublatti": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Art of Painting": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tropical kingbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "La bohème, Act II set design": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Kuchipudi Performer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Bindo Altoviti by Raphael": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Crepuscular rays (2019)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Zebra longwing butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Lilium bulbiferum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Shah Is Gone": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Kastellet, Stockholm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cartesian theater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "USS Enterprise bomb": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Cheetah portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sandow returns!": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "John Milton Brannan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Charles Hodge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "AAAAAAAAAHHHHHHHH!!!!": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Goethe in the Roman Campagna": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sedum acre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Noisy Friarbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of Marchesa Brigida Spinola-Doria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Yeongjo of Joseon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Battleship Row": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Brown-headed Honeyeater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Invasion of Grenada": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Clouded yellows mating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Featured picture candidates/Thalurania colombica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pura Besakih": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "James Birdseye McPherson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The Cleveland Orchestra": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Prothonotary Warbler (Audubon)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Li Fu Lee": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Jack Johnson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Aidan Gillen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Carmen Miranda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pancasila (politics)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Everglades": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Monroe Street Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "John Everett Millais - Mariana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Paul Delaroche - Bonaparte Crossing the Alps": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Billie Burke": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Grey-crowned babbler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chapel of St Leonard at Farleigh Hungerford Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Frida Kahlo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Marcos visit Reagan 1982": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Ada Flatman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Michael Giacchino": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Winding road to Shingo La": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Steppe buzzard": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Michael Ancher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Coronation of the Virgin (Velázquez)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Javan rusa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Eurasian blue tit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Colour balance (set)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "12P Pons–Brooks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Glanville fritillaries mating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Tale of the Bamboo Cutter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Messina's Portrait of a Man": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Anemonoides blanda": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Jeremiah Gurney": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Joseph Roswell Hawley": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Palm Springs International Airport ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "United States Capitol at dusk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Plains-wanderer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Ian Gillan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Wine barrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lwowska Galeria Sztuki by Jacek Malczewski": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Pallas' cat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Centaurea jacea": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "A Young Girl Reading": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Saint-Gaudens double eagle (Obverse)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mercedes-Benz W115": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Juvenile Nubian ibex": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Polar Bear jumping, in Spitsbergen Island, Svalbard, Norway.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Tomb of Mian Ghulam Kalhoro": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "T-7 Red Hawk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Gorgoneion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Milk Drop Coronet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Porters Pass": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of Leo X": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sea Girt Light": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cuban emerald": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hazelnut": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Birefringence": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Dindigul Rock Fort": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Melk Abbey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Napoleon Bonaparte": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Xbox One": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Dominostein": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Rebirth of Gulf Fritillary": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Aachen Cathedral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Burning of the USS Missouri": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Auxiliary Territorial Service women working on a Churchill tank at a Royal Army Ordnance Corps depot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "John McCain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Fires in Israel (October 7)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Miracle of Empel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Tiger under a pine tree": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Barn Owl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Spotted pardalote": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "John Rocque's Map of London, Westminster, and Southwark": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Köln Hauptbahnhof": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Construction of the World Trade Center": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Princess Augusta of Bavaria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gulf fritillary feeding on passion flower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "DeFord Bailey (improved)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Candy apple": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sunbird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Coal miners homes in Benito, Kentucky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The Miami Skyline Panorama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Napoleon Crossing the Alps (Belvedere version)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Crocus tommasinianus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Titanus giganteus beetle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Verizon Building after September 11, 2001": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Huntsman Spider RSA": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Meteorite in Miller Range blue-ice area": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Passiflora caerulea (blue passionflower) STEREO (R-L)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sedge warbler (Acrocephalus schoenobaenus) 3.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Balearic green toad": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mizrah": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Paul Fildes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Renominated chicken egg diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Boeing 747-400 contrails": {
    "yes": [
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Agile frog (Rana dalmatina)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "A Map of Old El Paso": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "1966 flood of the Arno": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "St Mary Magdalene Church, Sandringham": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Saint-Séverin ceiling": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Sword-billed hummingbird on branch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "NGC 7635 (2nd nomination)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Masjid Nabawi The Prophet's Mosque, Madina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The baroque garden at Drottningholm Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Aharbal Waterfall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Triton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cliff Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gillette de Narbonne": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Plantago lanceolata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Asics Gel-Cumulus 22 running shoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Považský hrad": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "God the Father - Arise": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Diver": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Sissieretta Jones": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Tunisian women's shoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Margaret D. Foster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Sarlyk Yak": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Skew lines": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Auriga and Aquarius": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mimetite": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Ama Dablam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hurricane Patricia of 2015 at its record peak intensity": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Papilio machaon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "OREOS!!!!!": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Comparison of notable bridges to scale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "NROL-39": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Village weaver male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eldridge Street Synagogue": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Revenue stamp for beer tax, 1871 issue": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Elliðaey (Breiðafjörður)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Volkswagen I.D. series Buggy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tsarskiy Kurgan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Charles Henry Turner": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Claude Monet 1899": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Yerevan Opera Theatre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ha-ha (seriously)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Monterde": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sun bear": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Aerial view of Tehran in 1925": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Prehnite": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Canary Wharf": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Venus with a Mirror": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hazel MacKaye": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Blue Marble (rescanned and color-calibrated)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Thomas Browne with his wife Dorothy, by Joan Carlile": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Obelisco (Obelisk), Buenos Aires, Argentina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Papal States, Quadrupla Scudo d'Oro (1689) depicting Pope Alexander VIII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "A Boy with a Flying Squirrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Marsh fritillary male (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pieter Bruegel the Elder- The Harvesters": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Glassy carbon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "White-headed Petrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "California sea lion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Berlin Crisis of 1961": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Saltwater limpet diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Virginia Woolf": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Anthony Henday Drive": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Rose of Persia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "SBB Red Arrow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Brown pelican - in flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Tobacco Mosaic Virus diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "A map of interwar Europe, 1923": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Portrait of Margaret van Eyck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Paralepista flaccida": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cannon diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Comparison of orbits around Earth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Red-eyed dove": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Aldrin looks back at Tranquility Base": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Spotted fritillary (Melitaea didyma) underside Macedonia.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Venus's car, bleeding heart or lyre flower (Dicentra spectabilis) is a plant from the family of the Fumariaceae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ruins of Agdam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Aurora borealis over Víkurkirkja church": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Atractomorpha crenulata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Jean-François Millet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "David Copperfield": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Skyline of Datong, China": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Parson's chameleon female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Tensai Okamura": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Australian ibis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Oscar Wilde": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Graziella": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sulfur (2021)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Palau de la Música Catalana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Matteuccia struthiopteris fiddleheads": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Female Common chimpanzee": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "End of World War II in Finland": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Sultan Ahmed Mosque": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sunset lorikeet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Pipe organ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Isambard Kingdom Brunel Standing Before the Launching Chains of the Great Eastern": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Juvenile coot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Manila Central Post Office": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Charles Catton, by Charles Catton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Joy Young Rogers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Sarah Bernhardt by Nadar 1864": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "ColecoVision 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Jeanette Scissum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Bloody Saturday": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Thomas Mundy Peterson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Florence Cathedral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Orange cup coral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Deception I - Jim the Penman (Emanuel Ninger)": {
    "yes": [
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Academy of Athens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Conspiracy of Claudius Civilis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Museu de les Ciències Príncipe Felipe": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Haboku sansui (Broken Ink Landscape scroll)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Kublai Khan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Violet dropwing female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Barnard 33": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dark blue tiger butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "John William Waterhouse - The Lady of Shalott": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Nueva Esparta geographic map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Hemiolaus cobaltina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Daniel Boone at Cumberland Gap": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ferdinand VII of Spain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Franz Lehár": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Flatten the curve": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Two cheetah brothers cleaning each other": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Woodbury Langdon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bodie County Barn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Socially Distanced Homeless Encampment": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Tatting": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Basking shark": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mario Gotze": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Meditation by Lang Jingshan": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [
      "gpt-4o-2024-05-13",
      "gpt-4o-mini-2024-07-18"
    ],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Palazzo Barberini (Rome) - Borromini's staircase": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Santa Maria della Vittoria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Aldrin Apollo 11 original": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Fire dartfish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Great Court of the British Museum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Look Back in Ingres...": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Quo Vadis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mexican vine snake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Erfurt 10 Ducat (1645) depicting Queen Christina of Sweden": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Julia Dujmovits at the Tag des Sports (Day of Sports) 2013 on Heldenplatz in Vienna, Austria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Battle of the Monitor and the Merrimac": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "September Morn": {
    "yes": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Portrait of Elena Grimaldi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "LAPD Ford Crown Victoria Police Interceptor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Here kitty kitty": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Macrocranion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Caves of Hercules": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "US Hurricane Damage - Waffle House Index": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Caroline Webster Schermerhorn Astor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Endomembrane system diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Large red damselfly female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "MS Birka Stockholm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "ZenFone 6": {
    "yes": [
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Golden Veranda in 2015": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Cotacotani Lakes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Green terror": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Indian Rabbit": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Soprano saxophone": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Daniel Bernoulli": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Mirabita Sisters by Antoine de Favray (1754/1764)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Peter Carey (historian)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Alexander Fleming": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Abyssinian black-and-white colobus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "draw",
    "num_of_highest_votes": 4
  },
  "Caroline Hill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Theloderma corticale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "2024 Lincoln, Nebraska EF3 tornado": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Menger sponge after 4 iterations": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Lilac-breasted roller": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "WASP": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Jewel Changi Airport Vortex": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ryan Cohen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Slosh dynamics": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "House in Provence": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lightning in Port-la-Nouvell": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Anna Fisher in Apollo Astronaut Spacesuit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mars meteorite structures": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "Helen Hunt Jackson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Fountain of Qayt Bay (Temple Mount, Jerusalem)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "First JATO assisted flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Artemis 2 Crew Portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Uppsala Cathedral": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Carabao Cart": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dorcus parallelipipedus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hawksmoor Towers, All Souls College, Oxford": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Donald Trump": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dracula's Daughter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Death's-head hawkmoth larva": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dois Irmãos": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Ebola virus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Taraxacum officinale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Édouard de Reszke": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Elizabeth II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Euphorbia milii flower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Forte Príncipe da Beira": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Helene Schjerfbeck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The church with a magnetic personality": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "U.S. Route 66": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hyacinth macaw in flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Lars Kruse": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Trillium erectum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Françoise Arnoul": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Skokloster Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Eva Le Gallienne": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Aularches miliaris": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Kadriorg Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Destroy this mad brute": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Midsummer Eve Bonfire on Skagen Beach": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Tadoba Andhari Tiger Project": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Paxillus involutus mushroom": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Day Dream": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sherlock Holmes and Moriarty fight to the death at the Reichenbach Falls": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Castle by the River": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Barber of Seville": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "An American locomotive transporting allied aid for Russia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Coronavirus rainbow window display": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "En pointe ballet shoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Louis Pasteur in 1885": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jacques Offenbach": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Kourion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Solar eclipse of August 21, 2017": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Men of the Docks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Susan B Anthony": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "\"An International High Noon Divorce\"": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Germanicus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Las Lajas Sanctuary": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gowanus Canal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Yellow-billed shrike": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Equestrian Portrait of Cornelis and Michiel Pompe van Meerdervoort with Their Tutor and Coachman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ashmont–Mattapan Line PCC streetcar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Kotor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Conus marmoreus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Soyuz rocket": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Aisha Moh’d Kazaure": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Leptosia nina nina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Woman lighting diyo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dallol": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Western bluebirds huddling in the cold": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Black kite in flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Absinthe makes the heart grow fonder": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Aeshna affinis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Adana massacre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Phineas Gage Cased Daguerreotype": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "W. H. Kendal as Philamir and Madge Kendal as Zeolide in W. S. Gilbert's The Palace of Truth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Pullman porter in advertisement": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Aerial Photo of The City of Dana Point, California": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Cathedral Ruins in Hamar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Mohiniyattam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Samuel Plimsoll": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Blue wildebeest males in duel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Uhlan of the Kharkov Regiment": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Borbo cinnara": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Shrovetide Revellers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bulb Fields": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "14th C. Doberan Minster altar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Siproeta stelenes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "2017 Women's March": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jennifer Granholm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "St. John the Baptist (Leonardo)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Three Philosophers by Giorgione. ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Nude Descending a Staircase, No. 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Russian battleship Dvenadsat Apostolov": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Paulette del Baye": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Gustave III, Act III": {
    "yes": [
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Nadar élevant la Photographie à la hauteur de l'Art": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Women basketball match in Alginet, 1956.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Thin section scan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Acer negundo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Le Grand Canal by Claude Monet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Striped albatross": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Church of St. John at Kaneo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Orange": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Roadside hawk with kill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Toni Morrison": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Galite coast": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Wiesen Viaduct": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Humanist Library of Sélestat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cypress Tree Byōbu": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gemsbok (male)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Firefighting": {
    "yes": [
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Francis Bacon by Reginald Gray": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Hardheads": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tiger beetle Lophyra": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Buddha Vairocana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Boston skyline from Longfellow Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Analcime": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Tancredo Neves": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Craigdarroch Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Apostlebird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cenk Uygur hosting The Young Turks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Fremantle Prison 1971": {
    "yes": [
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Northeaster ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "First Czechoslovakian banknote issue, 10 korun (1919, provisional)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Claude Debussy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Maria Amélia, Princess of Brazil": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Nelson's sparrow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Atlantis Paradise Island Royal Towers At Night": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "A591 road": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Monster Soup commonly called Thames Water": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Large Hadron Collider": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Apollo 11 in popular culture": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Surgeon Placing Suture": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "Japanese macaque juvenile yawning": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hohenschwangau Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Paul Lormier - Costumes for Hector Berlioz's Benvenuto Cellini (1838) - Un chef d'atelier (Bernardino). M. Ferdinand Prévost": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The writing is on the wall for Belshazzar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Naria poraria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Odiham Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Profile Portrait of a Lady": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Newscast": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Jesus image on church window": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Bohemia and Moravia – 1 Koruna (1939)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Queen Victoria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The First 1948 Deutsche Mark (East)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Neil Armstrong after moonwalk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "War elephant": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Isaac Levitan: Golden Autumn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hurricanes Katia, Irma, and Jose": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Khost children in 2010 (Afghanistan)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Potassium alum crystal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cappadocia Chimneys - DWiW": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Nice tramway": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Simeis 147": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "S/V Rembrandt van Rĳn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Red-browed Finch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Central Park Manhattan Island New York": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Victor Herbert's The Fortune Teller": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Duke, Lovell and Haise at the Apollo 11 Capcom, Johnson Space Center, Houston, Texas": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Blue-tailed bee-eater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Grand Prismatic Spring": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "La Mousmé": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Crepidotus variabilis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chugach State Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Perovskia atriplicifolia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Bandits Roost, 59 1⁄2 Mulberry Street": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Infanta Margarita Teresa in a Blue Dress": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Skating Minister": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Hooded mountain tanager": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Buddhist temple doorknob": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Three Beauties of the Present Day": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Miguel de Unamuno": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Glenn T. Seaborg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Moraine Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Slime mold": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  " Mary praying": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Black-fronted dotterel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Glyphoglossus molossus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Sharp-tailed Sandpiper": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mandala": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Two-tailed pasha": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Azurite": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "An Old Man and his Grandson, by Domenico Ghirlandaio.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Indonesian rice farmer": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Chain moray": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "\"Broke, baby sick, and car trouble!\"": {
    "yes": [
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Brett Kissel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Muskox": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Juanita Hall as \"Bloody Mary\" in South Pacific": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Pieta": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mae Jemison": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mauritius kestrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Bistorta officinalis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Glory of Florentine Saints on the dome in San Lorenzo (Florence)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Plasma globe": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The original Xbox 360": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Beautiful demoiselle male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Red grasshawk male": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Helenium autumnale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Golden anemone": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Spring Scattering Stars": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "African grey hornbill (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Julie d'Aubigny redux": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Denomination set – German papiermark of the Weimar Republic (1920–24)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Phra Phuttha Chinnarat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Neptune": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "St. Mary's Basilica, Kraków": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Two-tailed pasha (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Juma Mosque, Shamakhi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gerrit Dou": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Emerald damselfyy": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Corvus splendens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Wounded Angel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lagu Kenangan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "101955 Bennu": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Portrait of the Russian Ambassador, Prince Andrey Priklonskiy, Folio from the Davis Album by Aliquli Jabbadar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Simone Veil": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Edvard Munch - The Scream (pastel)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Main slab of Darwinius masillae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Södersjukhuset hospital": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bathsheba holding King David's letter by Willem Drost": {
    "yes": [
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Small pearl-bordered fritillary": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Drifting": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sonoma chipmunk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Flying gurnard": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cerianthus membranaceus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Jasper Francis Cropsey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Atmospheric opacity": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "View over Gstaad": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Dust storm approaching Stratford, Texas": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Verner von Heidenstam : Hans Alienus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Yellow-faced Honeyeater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Aucanquilcha volcano": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Crab spider with butterfly prey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Springtime": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Male Komodo dragon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cherry Resort inside Temi Tea Garden, Namchi, Sikkim": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Antônio Carlos Jobim": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Shockwave truck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Clevedon Pier": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "USS Essex (CV-9) - January 1960": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Gibson's albatross": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Radiated tortoise": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Götterdämmerung": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dordogne river": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Aurelia Henry Reinhardt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Archibald Sinclair, 1st Viscount Thurso": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gloucester Cathedral Cloister": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "PlayStation 4": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Solar System": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Young Man with a Skull": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pigeon Point Light Station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Grassmarket, Edinburgh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Emily Davison": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Nigeen Lake panorama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "New Zealand fur seal female with suckling pup": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Falka": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cleopatra's Needle in New York City": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Putra Mosque dome": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hell Gate Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Jun Takahashi dress for Undercover": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Poesaka Terpendam": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Golden earrings from Gyeongju": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rootabaga Stories frontispiece": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Francis Scott Key Bridge collapse": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Crown of the Andes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "NGC 6357": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Bush Lying in State": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Imelda Marcos": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Jay Gould": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Martand Sun Temple Panorama": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hermann Schwarz": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Male black darter dragonfly (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Bank of North America, 3 pence (1789)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Away with the green fairies": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "SpaceShipOne flight 17P": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "First Netherlands Indies gulden": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Holothuria arguinensis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Time zones": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Onésime Reclus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Supreme Court of the United Kingdom": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ogoy Island": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Lucy Arbell in Massenet's Thérèse": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "United Kingdom invocation of Article 50 of the Treaty on European Union": {
    "yes": [
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Costume design for Arac, Gunon, and Scynthius in Princess Ida (1884)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Roztochia Biosphere Reserve": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Discovery Moment": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gas mask": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Garden at Sainte-Adresse": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Mazafati": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Verinag Water Spring": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rainbow bee-eater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Odissi Perfomance": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Château de la Muette,": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Weaver building a nest (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "A village girl, Palangan, Kurdistan, Iran": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Marine officer candidate Marine Corps Base Quantico 2019": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mattie Edwards Hewitt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Stielers Handatlas 1891": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Great Hercules by Hendrick Goltzius": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Supranational Post-Soviet Bodies": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Tu-160 at the 2013 Moscow Victory Day parade": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "A. Hamid Arief": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Female galah in flight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hindenburg disaster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "House sparrow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Common cuttlefish": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Oath of the Emperor of Brazil": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Laupa Manor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Warbling white-eye": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Expulsion from the Garden of Eden": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "White Zombie poster 2": {
    "yes": [
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Hamengkubuwono VII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Abolhassan Banisadr": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cimetière des Rois": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Photograph of artist Ilya Bolotowsky and assistant John Joslyn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Another lady, another virginal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Smoke of a .45": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Lenin (nuclear-powered icebreaker)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lido Isle Newport Beach CA": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Rega air rescue": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gare du Nord": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Lahore Fort": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Charlottenburg Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Aletta Jacobs": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "John Snow's cholera map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The fantastic Scolopendra gigantea": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Fire-breasted flowerpecker": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eero Antikainen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Australian brushturkey female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "War pigeons": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Portrait of Amalie Zuckerlandl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Quedlinburg Abbey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Howard Thurston": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Handshake Alternatives": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Common walnut": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Diagram of a hypothetical ancestral mollusc": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Hook echo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Pantala flavescens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "John Milton Brannan (redux)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Gina Krog": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Grey-breasted mountain toucan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "White-naped Honeyeater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Eastern Yellow Robin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Howrah Bridge elevation and extent": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Cathedral (Siena) - Dome interior": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "U.S. Navy patrol boat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ruby Hirose": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mamoru Shigemitsu signs the Japanese Instrument of Surrender": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "BL 60-pounder gun": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "President Kennedy addresses nation on Civil Rights, 11 June 1963": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Southern crested caracara (Caracara plancus) in flight.JPG": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Fried Chicken": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "White-fronted bee-eater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Geisha": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mallard II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Quaker Guns": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Pripyat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Gorky Park (Moscow)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Zephyranthes Carinata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Evert Collier's painting Vanitas, candlestick, musical instruments, Dutch books, a writing set, an astrological and a terrestrial globe, all on a draped table": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Echmiatsin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Eyal Zamir": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Common ringed plover": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dial indicator": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "St Paul's during a special service in 2008": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The Alchemist Discovering Phosphorus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Alessandro Vittoria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Titian's Allegory of Prudence": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Masaccio's Madonna and Child": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "A Wee \"Scrap of Paper\" is Britain's Bond": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Annunciation, with Saint Emidius": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Glassy Tiger butterfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Uniform compound of four tetrahedra (UC23)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mariam Mosque prayer chamber": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Euthrix potatoria caterpillar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Roman amphitheatre of Italica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Chandra Shumsher Jang Bahadur Rana": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Phyang monastery, Ladakh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mudpot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Wood stork and Yacare caiman": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ruddigore revival, 1921": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Parazoanthus axinellae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Palácio do Planalto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Three-spot grass yellow/2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Maroon Bells": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Ghan line, Lake Eyre South": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Serbian Christmas Meal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Xiahe mandible": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "William Crooks": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "I vespri siciliani": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Scalloped yellow glider female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "An Officer of the Imperial Horse Guards Charging": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cherenkov radiation": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Cannikin Nuclear Device": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Common kingfisher": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "LED matrix (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "File:Ely Cathedral Octagon Lantern 3, Cambridgeshire, UK - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Diploria strigosa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Aral Sea in 1989 and 2008": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "White-tailed eagle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Accolade": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Summer Evening at Skagen. The Artist's Wife and Dog by the Shore": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "General Map of Switzerland": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Rambutans with seed": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Moon gate": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Wilhelm Keitel signing unconditional surrender": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Roan antelope": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Female Eurasian brown bear": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Camille Pissarro - Boulevard Montmartre, Spring": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Boeing E-3 Sentry": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Honeycomb of honey bees with eggs and larvae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Schoolgirls in Bamozai": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "White Tower of Thessaloniki": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Himalayan salt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Black-faced Woodswallow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Fringes of the Fleet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Yule log cake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Diagram of the Sun": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Deepwater Horizon fire 2": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Ulmus laevis flowers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Emma Hamilton as a young woman c. 1782, by George Romney": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Diagram of Jupiter": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "British Airways Boeing 767-300ER planform view": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Artamus superciliosus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Richard Gerstl": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ravi Coltrane": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "A well-lived in face": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "\"Il Signor Tambourossini\"": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Umberto Eco": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Ice speedway": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dinosaur Ridge in 2019": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sadhu": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Pioneer plaque": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Spear of Attack": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "St. Mark's Square at the Grand Canal Shoppes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "ATLAS particle detector": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Sumatran Ground-Cuckoo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Helleborus orientalis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Cheakamus Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Rev. Richard Brown": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "White-faced storm petrel": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "John Cage": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Marino Faliero": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mieke Wijaya": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hanover New Town Hall lobby": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cauliflower fungus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "C/2022 E3 (ZTF)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Abies pinsapo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Adour Mk 811": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dalmatian pelican": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Jacques Rivette Dreaming": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Europa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gettysberg Portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Upper Belvedere": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ronald McNair, Guion Bluford, and Fred Gregory": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Three Arch Bay & the rock pools, Laguna Beach, California.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "USS Wisconsin (BB-64) Post refit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Scarlet darter female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hyochangwon as Korea's first golf course": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Lietava Castle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Black rhinoceros": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Trillium undulatum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Donald Trump photo op at St. John's Church": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Nettie Stevens": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Belgian version of the Yellow Badge": {
    "yes": [
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Hakken": {
    "yes": [
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "The Battle of Lepanto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Lansdowne Heracles": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Alba Madonna": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Leica I, 1927, the first full format camera worldwide für 135 film": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Nine Sovereigns at Windsor for the funeral of King Edward VII": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Peace lily": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Kue": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Areni shoe": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "von Wille's Munchausen syndrome": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mary Magdalene and Jesus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "William I of the Netherlands": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Gravel pit in Denmark": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Wrestlers": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Grey crowned crane portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Paysandisia archon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Antonin Artaud": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "George N. Barnard": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Oak mazegill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Edgar Allan Poe (redux)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Franz Liszt by Nadar, March 1886": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dusky lory": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Country view of Alentejo, Portugal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Anne of Cleves 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Red-whiskered bulbul/2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Cardona, Spain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Heteractis magnifica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pink-necked Green Pigeon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Ixion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Emerald damselfly (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Sargent's Ribblesdale": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Castle Bravo \"Shrimp\" Device": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Blue Lake (South Australia)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Pauline Gracia Beery Mack": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "A U.S. Navy diver enters the water during a training evaluation": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Fuyu persimmon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "African buffalo with oxpecker (2)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Presidio of San Francisco": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Mahasattva of Truc Lam Coming out of the Mountain.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Apollo 11 launch with Saturn V rocket": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "JFK limousine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Medical examination during World War 1 at Washington, D.C.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Resurrection": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Verdi conducts Aida": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Palácio Imperial de Petrópolis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Kota Kinabalu City Mosque": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Chocó grasshopper (Opaon varicolor) male.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Orson Welles post-The War of the Worlds broadcast": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Marie-Aimée Roger-Miclos": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Hen Harlequin Duck": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hemimorphite": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Antonie Frederik Jan Floris Jacob van Omphal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Samuel Coleridge-Taylor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Gorakhpur Junction railway station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Endeavour docked to ISS": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Douglas A-20 Havoc": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Bleu de Gex 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Smailholm Tower": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Äußere Kanalstraße (KVB)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "File:Elephant Trunk.png": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "NGC 7635": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Goliath windmill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Banyunibo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Chelsea Manning": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Jabiru": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The boot is on the other foot": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Squaw Valley Ski Resort aerial tram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Mulberry Street (Manhattan)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cooch Behar Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Wat Phra Kaew": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Reichstag building, 2023": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Lynn Canyon Suspension Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Taractrocera ceramas": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Thor": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mia Farrow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bryum capillare leaf cells": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Thomas Jefferson Building": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "White-fronted honeyeater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "File:Guildhall, City of London - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "yes",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Unsinkable Molly Brown": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "wart-headed bug": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "British Robin": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Albert Einstein": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cyprus mouflon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Trump International Hotel and Tower, Chicago": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tirth Pat": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Airport crash tender": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Aviat Eagle II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cougar MRAP in test explosion": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "File:Guacamelee!_STCE_screenshot_B.JPEG": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Three-spot grass yellow": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "A Man Was Lynched Yesterday": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "Martin and Anna Bates": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Édouard Manet, A Bar at the Folies-Bergère": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Leptogorgia sarmentosa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Itsukushima Shrine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Symmetrical sash windows": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Downtown Miami Photo ": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Hurricane Florence": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Northwest Corner of the Forbidden City": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Atriolum robustum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Charles Dana Gibson (1902) Studies in expression. When women are jurors": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "M2 light tank": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Softball": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Enrique Pena Nieto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "The Last Hours of Abraham Lincoln": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Hummingbirds of Trinidad and Tobago": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Zimmerman Telegram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The White Knight One": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dost Mohammad Khan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Inside room of Teatro Colón, Buenos Aires, Argentina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ky-Mani Marley": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Origami cranes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "1974 Xenia tornado": {
    "yes": [
      "gpt-4o-2024-05-13"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Brisbane": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Frontispiece to The Pinafore Picture Book": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Seated Buddha": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "David Octavius Hill by Amelia Robertson Hill": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Stephen Colbert": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Albert Benjamin \"Happy\" Chandler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "General George Washington at Trenton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "The upper body of a P. m. malaccensis individual in Malaysia.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Snowbird Upper Middle and Lower Cirque": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Enceladus (mosaic)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Charles X of France": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Yad Vashem View": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Jahangirs Durbar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Eid al-Fitr prayer at the Taj Mahal": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Common wombat juvenile": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Hairy dragonfly": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Rubidium and caesium crystals": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Larry Adler": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Gi Gi the Sea Lion": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Henry David Thoreau": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Ontario's longest rock cut": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "MV Viking Sky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Colorful dreidels": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Hideo Kojima": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Force India VJM08 / Nico Hülkenberg at 2015 Japanese Grand Prix": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "World War I Recruitment Poster": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Chair": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Bishop Petros with Saint Peter the Apostle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Cosy Corner by Carl Larsson - 1893": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Tatev Monastery": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "A School in Urmia (1910)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Fishermen at work": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Subducting tectonic plate": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Sheikh Mansour Leghaei": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Blue Lagoon (geothermal spa)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Liberty atop the Freedom Monument, Riga": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Les Progrès de l'amour : Le rendez-vous by Jean-Honoré Fragonard (from 1771 until 1773)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Fish farm": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "GNOME": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Guernsey by Sentinel-2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "F-4 Phantom II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Structure of the Earth": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Kargah Buddha": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Big Ben": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Empress Joséphine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Great Pyrenees Mountain Dog": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "La Jolla, California": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Aphrophora alni mating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Mekong River": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Very Large Telescope (May 2021)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "White-winged forest sylph": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Apple pie with ice cream": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Sandhya Jane": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "20140402 MCDAAG Jahlil Okafor dunk": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Alpenglow in Coloarado": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Star trail": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Bernina Express": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Pierre-Joseph Proudhon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "TAAR1–Dopamine neuron pharmacodynamics diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Dürer's Rhinoceros": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Graham cracker (Graham-Cracker-Stack.jpg)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Samragyee RL Shah": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Stowage of the British slave ship Brookes": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Gagea minima": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Russian assignat-50 Rubles (1807)": {
    "yes": [
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Buffalo in Baluran National Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Durga Puja": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Two natives and canoes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Santa Maria in Vallicella (Rome) - Intern": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Nic Stone": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Gnesta, c. 1900": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "MCDAAG MVP Michael Porter Jr.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "From Amer Fort": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Garachico": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "James Hetfield with Metallica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Celeste": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Venetian Lagoon from Space": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Rare Sawfish at Atlantis Paradise Island": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Marine salvage": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Ibrahim Isaac Sidrak": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Eastern wood pewee": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Strolling Actresses Dressing in a Barn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Yakovlev Yak-11 \"Czech Mate\" 2014 gold heat @ close to 500mph during the Reno Air Races 2014": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Reichenstein Castle (Arlesheim)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Girl in a White Kimono": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Michael Joseph Savage": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ladies' Finger cross section": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Common leopard gecko": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Scilla siberica": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Portrait of a Lady": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Abu Dhabi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Galactic Center composite image": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "David Ben-Gurion proclaiming the Israeli Declaration of Independence": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Taylor Highway": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Shakuntala by Raja Ravi Varma": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Tombstone, AZ, in 1940": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Compañía de Jesús, Quito": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Napoleon III": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Arrest of Emmeline Pankhurst": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Portrait Of Boy (Frankie, The Organ Boy)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Bernou's map of Nothern America": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "2021 Table Mountain fire": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "The Sisters On Exibit": {
    "yes": [],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 9
  },
  "1794 Samuel Dunn Map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Epitonium scalare": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Dome in the interior of the Jameh Mosque of Isfahan, Isfahan, Iran": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Percy Grainger": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Clanculus ormophorus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Trevi Fountain": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Roger Puta 8 Canadian National Freights in Alberta": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Bryant Park frozen fountain (Josephine Shaw Lowell Memorial Fountain)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Polarised London Sky Pool": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "African bush elephant and calf": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mauro Castillo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Abbey Lincoln": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Betty Friedan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Lake Bled": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Aftermath of the 2011 Tōhoku earthquake and tsunami": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Persepolis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Willa Beatrice Brown": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Langley Covered Bridge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Franklin Pierce": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Black Amber Plum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Saint-Étienne-du-Mont": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Entrance and cross, Valle de los Caídos (Valley of the Fallen)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Kritonios Crown": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Verinag Mughal Garden": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "St. Pierre Cathedral, Geneva": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Jasenovac concentration camp": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "File:St Giles in the Fields Church, London - Diliff.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Metellus Raising the Siege by Armand-Charles Caraffe": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "The Who": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Juan Antonio Samaranch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Maratus volans": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Grey Go-away-bird": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Copenhagen Airport": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Gonepteryx rhamni": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Motherhood in the Philippines": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "American Badger (Taxidea taxus)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Georgette Seabrooke": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Puliyanthivu 3D map": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Whisky glass": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Ali Farka Touré": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "A pyrosome colony washed up into a California tide pool": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Chestnut-naped antpitta": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "F7F Tigercat N747MX La Patrona 2014 Reno Air Races": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Coal tipple in Kenvir, Kentucky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Long-tailed fiscal, Lanius cabanisi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Odessa Opera and Ballet Theater": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Rocky Point State Park": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cutaway of a Water Turbine Generator": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Black-headed Ibis": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Snæfellsjökull": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gustav III's masquerade outfit": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Tove Lo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Berca Mud Volcanoes 2": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Montreal Stock Exchange": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Shirley Temple": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Jens Filbrich": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Castel Sant'Angelo (new nomination)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gilpinia hercyniae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Magellanic penguin skeleton": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Joan Sebastian": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mi-gyaung": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Steven Spielberg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Merkel-Raute": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Faith Bacon": {
    "yes": [
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [
      "gpt-4o-mini-2024-07-18"
    ],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "A Woodburytype carte de visite photograph of Charles Darwin, published by John G. Murdoch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Microalgae": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Kinner Kailash range and Kalpa": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Shirley Bassey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Voltairine de Cleyre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "The interior of Deir-e Gachin Caravansarai": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "ESDPES": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Mahmoud Ahmadinejad": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Serge Gainsbourg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Mediterranean": {
    "yes": [
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Keokradong peak in Bandarban, Bangladesh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "The Hubble Space Telescope": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Cotton picker": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Carlotta 2018-06-18 1725Z": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Glitter bombing": {
    "yes": [
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "St. Sebastian (Antonello da Messina)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Pont des Arts, Paris": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Castle Gwynn": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "European green toad": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Great Mosque of Central Java (Interior)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Leigh Manson": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Mary Magdalene": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Small red damselfly female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Amputation": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Lorde (2014)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Francis II & I, Holy Roman Emperor.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Meat Börek (a.k.a. burek), family of pastries or pies found in the Balkans, Middle East and Central Asia": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Company houses for miners in Benito, Kentucky": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Six-banded armadillo": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Diagram of the Moon": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Male sapphire bluet": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Eternal Flame at the Armenian Genocide Memorial in Yerevan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "African Grey Hornbill female": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Map of Iran and Turan in Qajar dynasty drawn by Adolf Stieler - 1891": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Aeroflot Tupolev Tu-154M": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Poster by Bissill commissioned for the Ministry of Information": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Sarcophyton glaucum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Australian Cattle Dog with injured leg": {
    "yes": [
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 8
  },
  "Beatrix of the Netherlands": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Full scale test of ASROC nuclear depth charge": {
    "yes": [
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Persian warriors, possibly Immortals, a frieze in Darius's palace at Susa. Silicious glazed bricks, c. 510 BC, Louvre": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Patrick Rothfuss": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Chinese New Year Fashion Show": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "2023 Amory tornado": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Deadvlei": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Balisto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cliff overlooking sea - Great Australian Bight Commonwealth Marine Reserve": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Fire blight": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mauricio Macri, current President of Argentina": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Swami Agnivesh": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "File:Waymo self-driving car front view.gk.jpg": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Addition to Turgot map of Paris set": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Tbilisi skyline": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Lisa Gordon-Hagerty": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Orion Molecular Cloud complex": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Panorama of Udaipur": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "North-South Lake": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Škoda Superb": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "White-winged Triller": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Saguaro Sunset": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Seattle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Keys to the city": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Ostrich portrait": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Cathedral-Basilica of Our Lady of the Pillar": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Gerewol maiden": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Antiopella cristata": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mayon Volcano overlooking Legazpi City at Night": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Orange-winged parrots": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Mungo Park cover 1859": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Views of Wieliczka Salt Mines by Willem Hondius": {
    "yes": [
      "gpt-4o-2024-05-13",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Hydrogen wave function": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Drone fly on ragwort": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "The Cenotaph, Whitehall": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Marcos Clark Air Base 1979": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Inside Empty Liquid Asphalt Barge": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "The Angelus": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Golestan Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Dinosaur Ridge at Seoraksan": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Alex Salmond": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Petrillo Music Shell and Chicago Skyline": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Mir Space Station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Tarsier Hugs Mossy Branch": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Washington Crossing the Delaware": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Léon Blum": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Corvus splendens eating": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "African helmeted turtle": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "A Meat Stall with the Holy Family Giving Alms - Pieter Aertsen": {
    "yes": [
      "gpt-4o-2024-05-13",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "moondream2"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Chromodoris lochi": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Heteropoda venatoria": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Release of American POWs": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Bombing of Frankfurt am Main in World War II": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "Vladimir Palace": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Wood engraving Crucifixion of Jesus 1866 by Gustave Doré.": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Crossing the Rhine": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "Geometric moray": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "3 cent US postage stamp, scott cat no. 853, issued in 1939 for New York World's Fair": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Richard Rhodes": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 5
  },
  "Pumps at Dunball February 2014": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "U.S. Navy Task Force 38": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 6
  },
  "Wild Turkey": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Towers of Rouen Cathedral, west view": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Higashiyama-ku, Kyoto": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 8
  },
  "Santa Maria della Vittoria in Rome - Interior": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Llama-3.2-11B-Vision-Instruct",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Harrison Schmitt": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Digestive system diagram": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  },
  "African leopard": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 7
  },
  "Ashalim Power Station": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Llama-3.2-11B-Vision-Instruct",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "llava-v1.6-mistral-7b-hf",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 6
  },
  "Horseshoe Bend": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Battle of Manila Bay (1898)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Molmo-7B-D-0924"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 7
  },
  "St. George's Basilica, Malta (ceiling)": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "Qwen2-VL-7B-Instruct",
      "moondream2",
      "Molmo-7B-D-0924",
      "llava-1.5-7b-hf",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "yes",
    "num_of_highest_votes": 9
  },
  "Bhimbetka rock shelters": {
    "yes": [
      "gpt-4o-2024-05-13",
      "Qwen2-VL-7B-Instruct",
      "Molmo-7B-D-0924",
      "gpt-4o-mini-2024-07-18"
    ],
    "no": [
      "Phi-3.5-vision-instruct",
      "Llama-3.2-11B-Vision-Instruct",
      "llava-v1.6-mistral-7b-hf",
      "moondream2",
      "llava-1.5-7b-hf"
    ],
    "failure": [],
    "dataset": "featured_pictures_candidates",
    "registration": "no",
    "judge_of_majority_rule": "no",
    "num_of_highest_votes": 5
  }
}